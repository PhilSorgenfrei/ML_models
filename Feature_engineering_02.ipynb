{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering with tensorflow (notebook 02)\n",
    "\n",
    "These are my personal notes on the \"google cloud - feature engineering\" course on coursera (https://www.coursera.org/learn/feature-engineering). This notebook will continue where notebook 1 on this repo left off. Based on the same housing price dataset, this notebook will cover feature crossing and embeddings in tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "We will compare (the evaluation set loss of) **four** models for predicting house prices, where the first model serves as baseline for the others: \n",
    "\n",
    "1. Linear regressor // Data: house location (bucketized longtitude and latitude), median income of inhabitants, house properties (i.e., median age, rooms per house, bedrooms per room)\n",
    "2. Linear regressor // Data: Adding longitude x latitude feature crosses \n",
    "3. Linear regressor // Data: Substituting feature crosses with embeddings of longitude x latitude feature crosses  \n",
    "4. DNN regressor    // Data: Substituting feature crosses with embeddings of longitude x latitude feature crosses  \n",
    "\n",
    "We will use four general functions: \n",
    "\n",
    "- **add_features(df):** can be used to add additional features to the dataset (i.e., by combining existing features)\n",
    "- **make_input_fn(df, num_epochs):** creates a node in the comp graph that feeds the data. It calls add_features(df)\n",
    "- **create_feature_cols():** defines which features are passed to the model (and does some feature transformation, like one-hot-encoding) \n",
    "- **train_and_evaluate(output_dir, num_train_steps):** runs training and evaluation when called. Instantiates a model (linear regressor or DNN regressor) and calls the previous three functions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief look at the data again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages \n",
    "import itertools\n",
    "import tensorflow as tf \n",
    "import tensorflow.feature_column as fc \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data into a pandas dataframe\n",
    "df = pd.read_csv(\"data\\california_housing_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns in the dataset\n",
    "- **longitude and latitude** -- long and lat values for the US West Coast area\n",
    "- **housing_median_age** -- median age of houses in the area\n",
    "- **total_rooms** -- total number of rooms of all houses in the area \n",
    "- **total_bedrooms** -- total number of bedrooms of all houses in the area\n",
    "- **population** -- total number of people living in the area\n",
    "- **households** -- total number of households in the area\n",
    "- **median_income** -- median income \n",
    "- **median_house_value** -- **value to be predicted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-114.31</td>\n",
       "      <td>34.19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5612.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>1.4936</td>\n",
       "      <td>66900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-114.47</td>\n",
       "      <td>34.40</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7650.0</td>\n",
       "      <td>1901.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>1.8200</td>\n",
       "      <td>80100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-114.56</td>\n",
       "      <td>33.69</td>\n",
       "      <td>17.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.6509</td>\n",
       "      <td>85700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-114.57</td>\n",
       "      <td>33.64</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>3.1917</td>\n",
       "      <td>73400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-114.57</td>\n",
       "      <td>33.57</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>1.9250</td>\n",
       "      <td>65500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -114.31     34.19                15.0       5612.0          1283.0   \n",
       "1    -114.47     34.40                19.0       7650.0          1901.0   \n",
       "2    -114.56     33.69                17.0        720.0           174.0   \n",
       "3    -114.57     33.64                14.0       1501.0           337.0   \n",
       "4    -114.57     33.57                20.0       1454.0           326.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0      1015.0       472.0         1.4936             66900.0  \n",
       "1      1129.0       463.0         1.8200             80100.0  \n",
       "2       333.0       117.0         1.6509             85700.0  \n",
       "3       515.0       226.0         3.1917             73400.0  \n",
       "4       624.0       262.0         1.9250             65500.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A brief look at the summary statistics\n",
    "- no empty cells (17,000 entried for each column) \n",
    "- house values range between USD 15,000 - 500,000\n",
    "- longitude values range from -124 to -114\n",
    "- latitude values range from 32.5 to 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17000.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.562108</td>\n",
       "      <td>35.625225</td>\n",
       "      <td>28.589353</td>\n",
       "      <td>2643.664412</td>\n",
       "      <td>539.410824</td>\n",
       "      <td>1429.573941</td>\n",
       "      <td>501.221941</td>\n",
       "      <td>3.883578</td>\n",
       "      <td>207300.912353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.005166</td>\n",
       "      <td>2.137340</td>\n",
       "      <td>12.586937</td>\n",
       "      <td>2179.947071</td>\n",
       "      <td>421.499452</td>\n",
       "      <td>1147.852959</td>\n",
       "      <td>384.520841</td>\n",
       "      <td>1.908157</td>\n",
       "      <td>115983.764387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>14999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.790000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1462.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>790.000000</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>2.566375</td>\n",
       "      <td>119400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.490000</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2127.000000</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>3.544600</td>\n",
       "      <td>180400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.000000</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3151.250000</td>\n",
       "      <td>648.250000</td>\n",
       "      <td>1721.000000</td>\n",
       "      <td>605.250000</td>\n",
       "      <td>4.767000</td>\n",
       "      <td>265000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>37937.000000</td>\n",
       "      <td>6445.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>6082.000000</td>\n",
       "      <td>15.000100</td>\n",
       "      <td>500001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  17000.000000  17000.000000        17000.000000  17000.000000   \n",
       "mean    -119.562108     35.625225           28.589353   2643.664412   \n",
       "std        2.005166      2.137340           12.586937   2179.947071   \n",
       "min     -124.350000     32.540000            1.000000      2.000000   \n",
       "25%     -121.790000     33.930000           18.000000   1462.000000   \n",
       "50%     -118.490000     34.250000           29.000000   2127.000000   \n",
       "75%     -118.000000     37.720000           37.000000   3151.250000   \n",
       "max     -114.310000     41.950000           52.000000  37937.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    17000.000000  17000.000000  17000.000000   17000.000000   \n",
       "mean       539.410824   1429.573941    501.221941       3.883578   \n",
       "std        421.499452   1147.852959    384.520841       1.908157   \n",
       "min          1.000000      3.000000      1.000000       0.499900   \n",
       "25%        297.000000    790.000000    282.000000       2.566375   \n",
       "50%        434.000000   1167.000000    409.000000       3.544600   \n",
       "75%        648.250000   1721.000000    605.250000       4.767000   \n",
       "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
       "\n",
       "       median_house_value  \n",
       "count        17000.000000  \n",
       "mean        207300.912353  \n",
       "std         115983.764387  \n",
       "min          14999.000000  \n",
       "25%         119400.000000  \n",
       "50%         180400.000000  \n",
       "75%         265000.000000  \n",
       "max         500001.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train and evaluation set \n",
    "def split_train_dev(train_split, df):\n",
    "    train_df = df.sample(frac=train_split,random_state=1)\n",
    "    dev_df = df.drop(train_df.index)\n",
    "    return train_df, dev_df\n",
    "\n",
    "train_df, eval_df = split_train_dev(0.8, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the linear and dnn regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHARED INPUT FUNCTIONS \n",
    "\n",
    "# input function \n",
    "def make_input_fn(df, num_epochs):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x = add_features(df),\n",
    "        y = df['median_house_value'] / 100000, # !!! \n",
    "        batch_size = 128,\n",
    "        num_epochs = num_epochs,\n",
    "        shuffle = True,\n",
    "        queue_capacity = 1000,\n",
    "        num_threads = 1\n",
    "    )\n",
    "\n",
    "# LINEAR REGRESSOR \n",
    "\n",
    "# Create estimator train and evaluate function\n",
    "def linear_train_and_evaluate(output_dir, num_train_steps):\n",
    "    \n",
    "    # Specify output directory  \n",
    "    run_config = tf.estimator.RunConfig(\n",
    "                 model_dir=output_dir,      \n",
    "                 save_summary_steps=100,                       \n",
    "                 save_checkpoints_steps=100)   # dictates max frequency of eval \n",
    "    \n",
    "    # specify model \n",
    "    estimator = tf.estimator.LinearRegressor(config=run_config,\n",
    "                                             feature_columns = create_feature_cols())\n",
    "    \n",
    "    # specify train set\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(train_df, None), \n",
    "                                             max_steps = num_train_steps)\n",
    "    \n",
    "    # specify eval set \n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(eval_df, 1), \n",
    "                                    steps = None, \n",
    "                                    throttle_secs = 5)  # evaluates no more than every 5 seconds per second\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "# DNN REGRESSOR \n",
    "\n",
    "def dnn_train_and_evaluate(output_dir, num_train_steps):\n",
    "    \n",
    "    # Specify output directory  \n",
    "    run_config = tf.estimator.RunConfig(\n",
    "                 model_dir=output_dir,      \n",
    "                 save_summary_steps=100,                       \n",
    "                 save_checkpoints_steps=100)   # dictates max frequency of eval \n",
    "    \n",
    "    # specify model \n",
    "    estimator = tf.estimator.DNNRegressor(hidden_units=[75, 25, 7],\n",
    "                                          config=run_config,\n",
    "                                          feature_columns = create_feature_cols(), \n",
    "                                          batch_norm=True)\n",
    "    \n",
    "    # specify train set\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(train_df, None), \n",
    "                                             max_steps = num_train_steps)\n",
    "    \n",
    "    # specify eval set \n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(eval_df, 1), \n",
    "                                    steps = None, \n",
    "                                    throttle_secs = 5)  # evaluates no more than every 5 seconds per second\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature crosses\n",
    "\n",
    "**Idea:** Combine input features in such a way that the model does not have to explicitely learn their interactions/dependencies, but rather receives feature combinations as input. \n",
    "\n",
    "**For example,** in this dataset we have data for **latitude and longitude** for each sample. We then proceeded and put these values into **discrete buckets** grouping floating point values together roughly every 0.5 degrees. During training, the model \"learns\" weights to multiply these input features with to optimize the prediction loss.  \n",
    "\n",
    "**The issue with this method:** Let's assume there is a certain quadrant of land (at longitude 110.0-110.5 / latitude 33.0-33.5) where property prices are extremely high. In this case, the model cannot simply assign a very high weight to the longitude 110.0-110.5 bucket if slightly to the north of the high-value quandrant (let's say at longitude 110.0-110.5 / latitude 34.0-34.5) house prices are for some reason low. In other words, **only the combination of these two specific coordinates indicates high prices in this case.** While a linear model will not be able to \"learn\" this, a deep neural network can, though at the cost of complexity (i.e., compute power). \n",
    "\n",
    "If we instead **create new feature columns,** where each column consists of the multiplication of two specific longitudes/ latitude buckets (i.e., output == 1 if a property happens to fall into that specific area), then the model can easily attach a weight to every single quadrant that is described in this way. This means that even a simple linear model can learn to predict high prices for some quadrants and lower prices for others.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images\\feature_cross.jpg' width='1000' height='1000'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus expect that the linear model will close the performance gap to the neural network in our example (and potentially for our neural net to converge faster and require less depth/complexity)\n",
    "\n",
    "**The risk of feature crosses:** model can overfit if we present too many cominations of the same data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature crosses in tensorflow\n",
    "\n",
    "The method **tf.feature_column.crossed_column([<cat_column>, <cat_column>], nbuckets)** requires a list of categorical input columns. The number of buckets determines onto how many buckets the resulting feature cross combinations will be distributed.* \n",
    "- If the number of buckets specified matches exactly the number of category combinations from the feature columns, then each combination will end up in one column. \n",
    "- If the number of buckets specified is smaller than the number of category combinations from the feature columns, then multiple combinations will fall into the same column. This forces the model to generalize more. If it is larger, samples from one combination will be spread across multiple columns. This allows the model for a higher degree of \"memorizing\" the training data \n",
    "\n",
    " \n",
    "*intuitively, the method creates a hash for each feature cross combination which it then divides by the number of buckets (using the modulo operation) which assign it to a bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model: Linear regressor without feature crosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating some input features \n",
    "def add_features(df):\n",
    "    df['avg_rooms_per_house'] = df['total_rooms'] / df['households'] #expect positive correlation\n",
    "    df['avg_persons_per_room'] = df['population'] / df['total_rooms'] #expect negative correlation\n",
    "    df['avg_bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms'] # expect negative correlation \n",
    "    return df\n",
    "\n",
    "\n",
    "# Defining which features to include and bucketizing longitude and latitude\n",
    "def create_feature_cols():\n",
    "    \n",
    "    # define number of longitude and latitude buckets\n",
    "    num_buckets = 30 \n",
    "    long_buckets = np.linspace(-124.0, -114.5, num_buckets).tolist()\n",
    "    lat_buckets = np.linspace(32.0, 42, num_buckets).tolist()\n",
    "    \n",
    "    # define input features \n",
    "    return [\n",
    "    fc.bucketized_column(tf.feature_column.numeric_column('longitude'), \n",
    "                                        boundaries = long_buckets),  \n",
    "    fc.bucketized_column(tf.feature_column.numeric_column('latitude'), \n",
    "                                        boundaries = lat_buckets),\n",
    "    fc.numeric_column('median_income'),\n",
    "    fc.numeric_column('housing_median_age'),\n",
    "    fc.numeric_column('avg_rooms_per_house'),\n",
    "    fc.numeric_column('avg_persons_per_room'),\n",
    "    fc.numeric_column('avg_bedrooms_per_room')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent verbose output\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "\n",
    "# run baseline model\n",
    "linear_train_and_evaluate(output_dir='CHECKPOINTS/feat_eng_02/model_base', num_train_steps = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the baseline loss achieved by the first model on the evaluation set\n",
    "\n",
    "<img src='images\\feature_eng2_base.PNG' width='600' height='600'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Linear regressor with feature cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are adding a feature cross between the bucketized values of longitude and latitude. Each new feature consists of a unique combination of longitude (e.g., 123.5-124) and latitude (e.g., 34.5-35) that in this case make up a physical area on a map. Each sample from the housing data falls into exactly one of those areas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new input features \n",
    "def add_features(df):\n",
    "    df['avg_rooms_per_house'] = df['total_rooms'] / df['households'] #expect positive correlation\n",
    "    df['avg_persons_per_room'] = df['population'] / df['total_rooms'] #expect negative correlation\n",
    "    df['avg_bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms'] # expect negative correlation \n",
    "    return df\n",
    "\n",
    "\n",
    "# Add feature cross\n",
    "def create_feature_cols():\n",
    "    num_buckets = 30 \n",
    "    long_buckets = np.linspace(-124.0, -114.5, num_buckets).tolist()\n",
    "    lat_buckets = np.linspace(32.0, 42, num_buckets).tolist()\n",
    "    \n",
    "    b_long = fc.bucketized_column(fc.numeric_column('longitude'), long_buckets)  \n",
    "    b_lat = fc.bucketized_column(fc.numeric_column('latitude'), lat_buckets)\n",
    "    \n",
    "    return [\n",
    "    # add feature cross\n",
    "    fc.crossed_column([b_lat, b_long], num_buckets**2), \n",
    "    # add other features \n",
    "    fc.numeric_column('median_income'),\n",
    "    fc.numeric_column('housing_median_age'),\n",
    "    fc.numeric_column('avg_rooms_per_house'),\n",
    "    fc.numeric_column('avg_persons_per_room'),\n",
    "    fc.numeric_column('avg_bedrooms_per_room')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model 2\n",
    "linear_train_and_evaluate(output_dir='CHECKPOINTS/feat_eng_02/model_2', num_train_steps = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the average loss output recorded during training (chart below), we see a clear improvement from adding the feature crosses (light blue line) relative to the original baseline (dark blue line)\n",
    "\n",
    "<img src='images\\feature_eng2_model_2.PNG' width='600' height='600'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why embeddings:** \n",
    "\n",
    "The issue with feature crosses is that they create a very **sparse encoding**, i.e., for each sample in the dataset we have hundreds of possible longitude / latitude bucket combinations, of which exactly one has a value of 1 (i.e., the bucket that correpsonds to the area where the house is in) while all the others have a value of 0. Embeddings translate this sparse representation into something more dense (and meaningful):\n",
    "\n",
    "Rather than feeding the regressor with all the hundreds of feature cross values, we run those through a dense layer with one or more neurons, which then feed into the network. Like any other parameter, the model trains the weights of this dense layer with respect to the objective function (i.e., in our case minimizing the loss from the difference between predicted and actual house prices).  \n",
    "\n",
    "Each embedding feature is a real floating point number (the weighted sum of feature crosses). Feature crosses (in our case areas of land) that are similar to each other in ways that determine house prices, will receive similar values from this embedding exercise. Crucially, when looking at a new sample, the weights applied to its location will make the model treat it similarly to other houses from the same location (at least with respect to the impact of location on house prices). \n",
    "\n",
    "Embeddings are a critical part to recommendation engines (e.g., movies on netflix) or natural language models (e.g., google translate).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituting the raw feature crosses with embeddings in our input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new input features \n",
    "def add_features(df):\n",
    "    df['avg_rooms_per_house'] = df['total_rooms'] / df['households'] #expect positive correlation\n",
    "    df['avg_persons_per_room'] = df['population'] / df['total_rooms'] #expect negative correlation\n",
    "    df['avg_bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms'] # expect negative correlation \n",
    "    return df\n",
    "\n",
    "\n",
    "# Add feature cross\n",
    "def create_feature_cols():\n",
    "    num_buckets = 30 \n",
    "    long_buckets = np.linspace(-124.0, -114.5, num_buckets).tolist()\n",
    "    lat_buckets = np.linspace(32.0, 42, num_buckets).tolist()\n",
    "    \n",
    "    b_long = fc.bucketized_column(fc.numeric_column('longitude'), long_buckets)  \n",
    "    b_lat = fc.bucketized_column(fc.numeric_column('latitude'), lat_buckets)\n",
    "    \n",
    "    feature_cross = fc.crossed_column([b_lat, b_long], num_buckets**2)\n",
    "    \n",
    "    return [\n",
    "    # add embedding\n",
    "    fc.embedding_column(feature_cross, num_buckets//4), \n",
    "    # add other features \n",
    "    fc.numeric_column('median_income'),\n",
    "    fc.numeric_column('housing_median_age'),\n",
    "    fc.numeric_column('avg_rooms_per_house'),\n",
    "    fc.numeric_column('avg_persons_per_room'),\n",
    "    fc.numeric_column('avg_bedrooms_per_room')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Linear regressor with embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model 3\n",
    "linear_train_and_evaluate(output_dir='CHECKPOINTS/feat_eng_02/model_3', num_train_steps = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a very similar outcome as in our model 2. However, converting sparse input feature into dense input features through embeddings, **we can run the dataset now through a deep neural network with tensorflow - see model 4 below**\n",
    "\n",
    "<img src='images\\feature_eng2_model_3.PNG' width='600' height='600'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: DNN regressor with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model 4\n",
    "dnn_train_and_evaluate(output_dir='CHECKPOINTS/feat_eng_02/model_6', num_train_steps = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the embedding through a (somewhat arbirtrarily configured) four layer neural network improves again on the previously observed loss from the linear regressor. The deep NN is able to better capture non-linearities in the input data which the linear model simply cannot capture\n",
    "\n",
    "<img src='images\\feature_eng2_model_4.PNG' width='600' height='600'/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
