{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for a multi-layer neural network for binary classification\n",
    "1. **Initialize parameters** (adjustable for Xavier and He initialization) -- initializes the layer parameters W and b and stores them in a dictionary\n",
    "2. **Initialize Adam** (Adaptive Moment Estimation) gradient descent -- initializes two dictionaries of the same shape as the parameters W and b to hold the momentum terms during the gradient descent. The first momentum term consists of the exponentially decaying average of past **squared** gradients (i.e., like RMSprop) and the second momentum term consists of an exponentially decaying average of past gradients (i.e., like momentum) \n",
    "3. **Linear forward propagation for one layer** -- computes the linear term Z for a layer based on the activation of the previous layer and the parameters W and b \n",
    "4. **Activation functions** -- encompasses functions to compute Sigmoid, tanh, ReLU, and leaky ReLU activations\n",
    "5. **Activation for one layer** integrating the linear forward function and the activation functions to compute the activation for one layer based on the activation of the previous layer, the parameters W and b, as well as the choice of activation function. Also implements the option for dropout to prevent overfitting \n",
    "6. **Forward propagation** -- computes one forward pass through the network based on the feature vector input X. Adjustable for Sigmoid, tanh, ReLU and leaky ReLU activation for hidden layers\n",
    "7. **Compute cost\"** -- computes the cross-entropy loss for each sample and averages it over all samples in the set/batch. Also implements L2 regularization (i.e., adding the sum of squared W parameters to the cost function to force smaller range of W values and thus reduce overfitting) \n",
    "8. **Linear backward** -- for a single layer, computes the partial derivative terms dW, db and of the previous layer actiavtion dA_prev. dW is calculated with the L2 regularization term lambda. Also implements dropout (i.e., setting dW and db terms to zero for those neurons that were \"dropped out\" on the forward pass\n",
    "9. **Activation function derivatives** -- computes partial derivatives for Sigmoid, tanh, ReLU and leaky ReLU activation functions \n",
    "10. **Linear activation backward** -- integrates the computation of the partial derivative of the cost function with respect to the activation in a given layer. Can handle a choice of Sigmoid, tanh, ReLU and leaky ReLU activation functions \n",
    "11. **Backward propagation** -- Runs backward propagation through all layers \n",
    "12. **Update parameters** -- update parameters W and b using Adam \n",
    "13. **Predict labels** -- Predict labels based on input vector X and learned parameters W and b \n",
    "14. **Calculate accuracy** -- Calculate accuracy of predictions against label vector (i.e., for trainign and dev set) \n",
    "\n",
    "\n",
    "*functions mostly generealizations of code introduced in Andrew Ng's deep learn specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, init_tuning=1):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of randomly initialized parameters W and b\n",
    "    Arguments: \n",
    "        layer_dims  --  array containing the dimensions of each layer [l1, l2, ln]\n",
    "        init_tuning --  parameter that adjusts the initialization of W for each \n",
    "                        layer (Xavier initalization: 0.5, He initualization: 1)\n",
    "                    \n",
    "    Returns:\n",
    "        parameters -- dict consisting of randomly initialized parameters W and b                 \n",
    "    \"\"\" \n",
    "    parameters = {}\n",
    "    depth = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, depth): \n",
    "        n = layer_dims[l]\n",
    "        n_prev = layer_dims[l-1]\n",
    "        \n",
    "        # balance: W's decrease on average with increasing size of previous layer to limit the \n",
    "        # range of Z \n",
    "        balance = np.sqrt(2 * init_tuning / n_prev) \n",
    "        \n",
    "        parameters[\"W\" + str(l)] = np.random.randn(n, n_prev) * balance\n",
    "        parameters[\"b\" + str(l)] = np.random.randn(n, 1)\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initialize Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Adam parameter dictionaries to store trailing averages \n",
    "def initialize_Adam(parameters): \n",
    "    \"\"\"\n",
    "    Returns two dictionaries for storing the momentum terms for Adam gradient descent\n",
    "    Arguments: \n",
    "        parameters -- dict of parameters W and b \n",
    "    Returns:     \n",
    "        v -- dictionary with zero arrays of the same shape as parameters\n",
    "        s -- dictionary with zero arrays of the same shape as parameters\n",
    "    \"\"\"\n",
    "        \n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],\n",
    "         parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],\n",
    "         parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0],\n",
    "         parameters[\"W\" + str(l+1)].shape[1]))\n",
    "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0],\n",
    "         parameters[\"b\" + str(l+1)].shape[1]))\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. linear forward propagation for one layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    computes the linear term Z for a layer based on the activation of the previous layer \n",
    "    and the parameters W and b \n",
    "    \n",
    "    Arguments:\n",
    "        A-prev -- activations of previous layer (n, 1)\n",
    "        W -- parameters for current layer (m, n)\n",
    "        b -- parameters for current layer (m, 1)\n",
    "\n",
    "    Returns:\n",
    "        Z -- Linear product matrix for current layer\n",
    "    \"\"\" \n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    linear_cache = (A_prev, W, b)\n",
    "\n",
    "    return Z, linear_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid - covers range 0 <-> +1 - exagerates small differences in Z around zero. \n",
    "# Risk of saturation for large and small values of Z (i.e., diminishing slope)  \n",
    "def sigmoid(Z):\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    return A\n",
    "\n",
    "# tanh - same shape as sigmoid, but covering range -1 <-> +1 - more differentiation range\n",
    "# Risk of saturation for large and small values of Z (i.e., diminishing slope)  \n",
    "def tanh(Z):\n",
    "     \n",
    "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    return A\n",
    "\n",
    "# relu - sets all negative values to zero while leaving positive values unchanged\n",
    "def relu(Z):\n",
    "    \n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    return A\n",
    "\n",
    "# leaky relu - small psoitive slope for negative values \n",
    "def leaky_relu(Z):\n",
    "\n",
    "    A = np.where(Z > 0, Z, 0.01*Z)\n",
    "    assert (A.shape == Z.shape)\n",
    "    \n",
    "    return A      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Activation for one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_activation(A_prev, W, b, deep_activation, keep_prob):\n",
    "    \"\"\"\n",
    "    Computes the activation for a layer based on the activation of the previous layer, the parameters\n",
    "    W and b asn as well the choice of activation function\n",
    "    \n",
    "    Arguments:\n",
    "        A_prev -- activation of previous layer \n",
    "        W, b -- parameters of layer \n",
    "        deep_activation -- type of activation function for hidden units\n",
    "        keep_prob -- probablity that a node in a hidden layer will not be dropped\n",
    "    \n",
    "    Returns:\n",
    "        A -- Activation matrix of current layer \n",
    "    \"\"\"\n",
    "    \n",
    "    # use linear forward to calculate Z for current layer \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    # use activation\n",
    "    if deep_activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    elif deep_activation == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "        \n",
    "    elif deep_activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "        \n",
    "    elif deep_activation == \"leaky_relu\":\n",
    "        A = leaky_relu(Z)\n",
    "        \n",
    "    # Implement dropout: \n",
    "    # -- create a matrix (mask) of the same shape as A with random values between 0-1\n",
    "    D = np.random.rand(A.shape[0], A.shape[1])\n",
    "    # -- set values in D that are larger than keep_prob to zero\n",
    "    D = (D < keep_prob)   \n",
    "    # -- zero out nodes from A by multiplying with mask D \n",
    "    A = A * D\n",
    "    # -- rebalance A (so that the sum of all A terms roughly the same as without dropout)\n",
    "    A = A / keep_prob \n",
    "        \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (Z, linear_cache, D)\n",
    "        \n",
    "    return A, cache  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 forward propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, deep_activation, keep_prob): \n",
    "    \"\"\"\n",
    "    Implements forward propagation with adjustable activation function for hidden layers\n",
    "    \n",
    "    Arguments:\n",
    "        X -- input feature vector\n",
    "        parameters -- dictionary of W and b \n",
    "        deep_activation -- activation function to use for hidden layers \n",
    "        keep_prob -- probability of NOT dropping a node in a layer in a given forward pass\n",
    "    \n",
    "    Returns:\n",
    "        AL -- final layer activation (value between 0-1)\n",
    "        caches -- list of caches (for each layer: Z, W, b, and the momentum terms D)\n",
    "\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    L = len(parameters) // 2    # corresponds to number of layers NOT incl. input layer\n",
    "    A = X\n",
    "    \n",
    "    # forward prop for deep layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation(A_prev, \n",
    "                                    parameters[\"W\"+str(l)], \n",
    "                                    parameters[\"b\"+str(l)],\n",
    "                                    deep_activation,\n",
    "                                    keep_prob)\n",
    "\n",
    "        caches.append(cache)\n",
    "    \n",
    "    keep_prob_output = 1.\n",
    "        \n",
    "    # forward prop output layer\n",
    "    AL, cache = linear_activation(A, \n",
    "                                    parameters[\"W\"+str(L)], \n",
    "                                    parameters[\"b\"+str(L)],\n",
    "                                    \"sigmoid\",\n",
    "                                    keep_prob_output)\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Compute cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_costs(AL, Y, parameters, lambd): \n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss for each sample and averages it over all samples in \n",
    "    the set/batch\n",
    "    \n",
    "    Arguments: \n",
    "        AL -- final layer output \n",
    "        Y -- label vector \n",
    "        parameters -- W and b \n",
    "        lambd -- L2 regularization term \n",
    "        \n",
    "    Returns: \n",
    "        cost -- cost after forward pass\n",
    "        \n",
    "    \"\"\"    \n",
    "    m = Y.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # cost function without regularization \n",
    "    cross_entropy_cost = (-1./ m) * np.sum(np.multiply(Y, np.log(AL)) + \\\n",
    "                                           np.multiply((1-Y), np.log( 1-AL)))\n",
    "    \n",
    "    # regularization - 1) sum over all W terms across all layers\n",
    "    sum_W = 0 \n",
    "    for l in range(1, L+1):\n",
    "        sum_w = np.sum(np.square(parameters[\"W\" + str(l)]))\n",
    "        sum_W += sum_w \n",
    "    \n",
    "    # regularization - 2) compute regularization cost term \n",
    "    L2_regularization_cost = 1/m * lambd/2 * sum_W\n",
    "       \n",
    "    # combine cross entropy cost with L2 regularization cost     \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "\n",
    "    cost = np.squeeze(cost)     # make sure to only get a single number\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. linear backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache, D_prev, lambd, keep_prob):\n",
    "    \"\"\"\n",
    "    For a single layer, computes the partial derivative terms dW, db and of the \n",
    "    previous layer actiavtion dA_prev\n",
    "    \n",
    "    Arguments: \n",
    "        dZ -- derivative of Z with regards to the cost function \n",
    "        linear_cache -- contains activation of previous layer, as well as W, b of current layer\n",
    "        D_prev -- matrix with zeros and ones used to implement droput on the forward pass \n",
    "        lambd -- lambda term of L2 regularization \n",
    "        keep_prob -- probablity of NOT \"dropping a neuron\" in any given hidden layer \n",
    "\n",
    "    Returns: \n",
    "        dA_prev -- partial derivative of the previous layer activation \n",
    "        dW, db -- partial derivatives of the parameters W and b for the current layer \n",
    "    \n",
    "    \"\"\"\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]     # number of samples in set \n",
    "\n",
    "    dW = 1./m * np.dot(dZ, A_prev.T) + lambd/m *W\n",
    "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ) \n",
    "    \n",
    "    # shut down same neurons as in forward pass \n",
    "    dA_prev = dA_prev * D_prev\n",
    "    # rebalance dA_prev to not change overall sum of terms \n",
    "    dA_prev = dA_prev / keep_prob        \n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Activation function derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    backward propagation for a single RELU unit.\n",
    "    \"\"\"\n",
    "    # slope of dZ/dA is 1 for z>0 and 0 for z <= 0 and with chain rule dZ = dA*dZ/dA\n",
    "    dZ_dA = np.where(Z > 0, 1, 0) \n",
    "    dZ = dA * dZ_dA\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    backward propagation for a single SIGMOID unit\n",
    "    \"\"\"\n",
    "        \n",
    "    a = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * a * (1-a)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def tanh_backward(dA, Z): \n",
    "    \n",
    "    a = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    dZ = dA * (1 - a**2)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def leaky_relu_backward(dA, Z): \n",
    "    \n",
    "    # slope of dZ/dA is 1 for z>0 and 0.01 for z <= 0 and with chain rule dZ = dA*dZ/dA\n",
    "    dZ_dA = np.where(Z > 0, 1, 0.01) \n",
    "    dZ = dA * dZ_dA\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. linear activation backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, D_prev, deep_activation, lambd, \n",
    "                               keep_prob):\n",
    "    \"\"\"\n",
    "    Integrates the computation of the partial derivative of the cost function with respect \n",
    "    to the activation in a given layer. Can handle a choice of Sigmoid, tanh, ReLU and \n",
    "    leaky ReLU activation functions\n",
    "    \n",
    "    Arguments: \n",
    "        dA -- partial differential of current layer activation \n",
    "        cache -- contains Z of and activation of previous layer, as well as W, b of current layer\n",
    "        D_prev -- matrix with zeros and ones used to implement droput on the forward pass \n",
    "        deep_activation -- activation function for deep layers, i.e., Sigmoid, tanh, ReLU and l ReLU \n",
    "        lamb -- lambda term of L2 regularization \n",
    "        keep_prob -- probablity of NOT \"dropping a neuron\" in any given hidden layer \n",
    "    \n",
    "    Returns: \n",
    "        dA_prev -- partial derivative of the previous layer activation \n",
    "        dW, db -- partial derivatives of the parameters W and b for the current layer \n",
    "    \"\"\"\n",
    "    Z, linear_cache, D = cache\n",
    "    \n",
    "    if deep_activation == \"relu\":\n",
    "        dZ = relu_backward(dA, Z)\n",
    "        \n",
    "    elif deep_activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "        \n",
    "    elif deep_activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, Z)\n",
    "        \n",
    "    elif deep_activation == \"leaky_relu\":\n",
    "        dZ = leaky_relu_backward(dA, Z)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache, D_prev, lambd, keep_prob)\n",
    "       \n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, deep_activation, lambd, keep_prob):\n",
    "    \"\"\"\n",
    "    Runs backward propagation through all layers \n",
    "    \n",
    "    Arguments: \n",
    "        AL -- final layer activation (value between 0-1)\n",
    "        Y -- label vector \n",
    "        caches -- list of caches (for each layer: Z, W, b, and the momentum terms D)\n",
    "        deep_activation -- activation function for hidden layer  \n",
    "        lambd -- L2 regularization term \n",
    "        keep_prob -- probablity that a node in a hidden layer will not be dropped\n",
    "    \n",
    "    Returns:\n",
    "        grads -- dictionary of gradients for W and b (for all layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    # capture dW and db for each layer \n",
    "    grads = {}  \n",
    "    L = len(caches)     # equal to number of layers excl. input layer \n",
    "    \n",
    "    # calculate dcost/dAL\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # calculate and store gradients for final layer \n",
    "    current_cache = caches[L-1]     # e.g., the cache for layer 1 is at position 0\n",
    "    previous_cache = caches[L-2]\n",
    "    Z, linear_cache, D_prev = previous_cache\n",
    "    dA_prev, dW, db = linear_activation_backward(dAL, current_cache, D_prev, \n",
    "                                                 \"sigmoid\", lambd, keep_prob)\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    \n",
    "    # calculate and store gradients for hidden layers \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (e.g., RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        previous_cache = caches[l-1]\n",
    "        Z, linear_cache, D_prev = previous_cache\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\"+str(l+1)], \n",
    "                                    current_cache, D_prev, deep_activation, \n",
    "                                    lambd, keep_prob)\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l+1)] = dW\n",
    "        grads[\"db\" + str(l+1)] = db\n",
    "            \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_Adam(parameters, grads, v, s, t, learning_rate,\n",
    "                                beta1, beta2,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "        parameters -- dict with parameters W and b\n",
    "        grads -- dict with grads dW and db\n",
    "        v -- dict with trailing averages (stored as: dW1, db1, etc.)\n",
    "        s -- dict with trailing averages of the squared gradient (stored as: dW1, db1, etc.)\n",
    "        learning_rate -- the learning rate alpha\n",
    "        beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "        beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "        epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "        parameters -- dict containig upadted paramters \n",
    "        v -- updated Adam variable \n",
    "        s -- updated Adam variable\n",
    "    \"\"\"\n",
    "    t += 1\n",
    "    L = len(parameters) // 2                 \n",
    "    # open dictionaries for first set of real trailing averages \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {} \n",
    "    \n",
    "    # Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # compute trailing average of parameters \n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "        # Correct for bias (i.e., initialized trailing averages all zero) \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
    "        \n",
    "        # Moving average of the squared gradients \n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * np.square(grads[\"dW\" + str(l+1)])\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * np.square(grads[\"db\" + str(l+1)])\n",
    "        \n",
    "        # Compute bias-corrected second raw moment estimate\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2**t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2**t)\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)]+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)]+epsilon)\n",
    "    \n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Predict labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X, deep_activation):\n",
    "    \"\"\"\n",
    "    Predict labels based on input vector X and learned parameters W and b \n",
    "    \n",
    "    Arguments:\n",
    "        parameters -- a dict containing learned parameters  \n",
    "        X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "        predictions -- size (1, m)\n",
    "    \"\"\"\n",
    "    # make sure no nodes are dropped for prediction\n",
    "    keep_prob_predict = 1\n",
    "    \n",
    "    # Computes probabilities using forward prop, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    AL, caches = L_model_forward(X, parameters, deep_activation, \n",
    "                                 keep_prob_predict)\n",
    "    predictions = AL\n",
    "    predictions = np.where(predictions > 0.5, 1, 0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Calculate accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, Y): \n",
    "    \"\"\"\n",
    "    returns accuracy (in %) of predictions vs Y\n",
    "    \n",
    "    Arguments: \n",
    "        predictions - binary predictions (0, 1) of shape (1, m)\n",
    "        Y - binary truth vector (0, 1) of shape (m, 1)\n",
    "   \n",
    "    Return: \n",
    "        accuracy(in %)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    assert (predictions.shape == Y.shape)\n",
    "\n",
    "    \n",
    "    # returns matrix of size (1, m) where 0 if prediction == Y \n",
    "    Abs = abs(predictions - Y)\n",
    "    accuracy = (1 - (np.sum(Abs) / m)) * 100 \n",
    "    return accuracy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
